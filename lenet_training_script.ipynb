{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lenet_training_script.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgn-1YlWUnFi"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "import time\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/Saravanan/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apdZnC7tUnri",
        "outputId": "8fdab60f-af30-432e-a493-69a4bff32568"
      },
      "source": [
        "from datasets import return_dataset\n",
        "[[mnist_train, mnist_test], [fashionmnist_train, fashionmnist_test], [cifar10_train, cifar10_test]] = return_dataset()\n",
        "datasets = [[mnist_train, mnist_test], [fashionmnist_train, fashionmnist_test], [cifar10_train, cifar10_test]]\n",
        "\n",
        "from initializations import initialize, negative_model, distorted_model\n",
        "initlist = ['pre_trained', 'kaiming_uniform', 'kaiming_normal']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dfkHKMSUpJi"
      },
      "source": [
        "#trainer function\n",
        "\n",
        "import copy\n",
        "from os import path, mkdir\n",
        "\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25,\n",
        "                folder = \"/content/drive/MyDrive/Saravanan/training_data/\", save_epochs=[]):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "    train_acc_history = []\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:               \n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                  # Get model outputs and calculate loss\n",
        "                  # Special case for inception because in training it has an auxiliary output. In train\n",
        "                  #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                  #   but in testing we only consider the final output.\n",
        "                  outputs = model(inputs)\n",
        "                  loss = criterion(outputs, labels)\n",
        "\n",
        "                  _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                  # backward + optimize only if in training phase\n",
        "                  if phase == 'train':\n",
        "                      loss.backward()\n",
        "                      optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "            \n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "            elif phase == 'train':\n",
        "                train_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "        # saving requested epochs\n",
        "        if epoch in save_epochs:        \n",
        "          add = folder + \"epoch_\" + str(epoch) + '.pt'          \n",
        "          torch.save(model, add)\n",
        "          print(\"saved epoch\", epoch, 'on', add[-60:])\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history, train_acc_history\n",
        "\n",
        "def run_train(model, traindata, testdata, \n",
        "              lr=0.005, momemntum=0.9, epochsnum=20, batch_size=128, num_workers=2,\n",
        "              rootadd = \"/content/drive/MyDrive/Saravanan/training_data/\",\n",
        "              save_epochs=[]):\n",
        "\n",
        "  name = model.name + '_init=' + model.initname + \"_on_\" + traindata.name + '_' + str(epochsnum) + \"epochs\"\n",
        "  folderadd = rootadd + name\n",
        "  \n",
        "  \n",
        "  # check folder does not exsit\n",
        "  trynumber = 2\n",
        "  testadd = folderadd\n",
        "  while path.exists(testadd):\n",
        "    testadd = folderadd + '_try' + str(trynumber)\n",
        "    trynumber += 1\n",
        "  folderadd = testadd + \"/\"\n",
        "  mkdir(folderadd)\n",
        "\n",
        "  initadd = folderadd + \"initial_model.pt\"\n",
        "  torch.save(model, initadd)\n",
        "\n",
        "\n",
        "\n",
        "  trainloader = torch.utils.data.DataLoader(traindata, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=num_workers)\n",
        "  testloader = torch.utils.data.DataLoader(testdata, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=num_workers)\n",
        "  dataloaders_dict = {'train': trainloader, 'val': testloader}\n",
        "  \n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momemntum)\n",
        "  \n",
        "\n",
        "  model.to(device)\n",
        "  print('\\n' * 3)\n",
        "  print(name + '\\n' + '='*len(name))\n",
        "  model, test_history, train_history = train_model(model=model, dataloaders=dataloaders_dict,\n",
        "                    criterion=criterion, optimizer=optimizer, num_epochs=epochsnum,\n",
        "                    folder=folderadd, save_epochs=save_epochs)\n",
        "\n",
        "  model.traindata = traindata.name\n",
        "  model.train_history = train_history\n",
        "  model.test_history = test_history\n",
        "  model.epochs_trained = epochsnum  \n",
        "\n",
        "  # save model\n",
        "  modeladd = folderadd[:-1] + \".pt\"\n",
        "  torch.save(model, modeladd)\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lenet import LeNet\n",
        "# train models\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "initlist = ['kaiming_uniform', 'kaiming_normal']\n",
        "\n",
        "epochsnum = 20\n",
        "lr = 0.005\n",
        "momemntum=0.9\n",
        "batch_size=512\n",
        "num_workers=2\n",
        "save_epochs = [x for x in range(epochsnum) if x%5 == 0]\n",
        "\n",
        "features_size = 512\n",
        "classes_size = 10\n",
        "\n",
        "for initname in initlist:\n",
        "\n",
        "    model = LeNet(classes_size)\n",
        "    model.name = 'lenet'\n",
        "    # model.fc = torch.nn.Linear(features_size, classes_size, bias=True)\n",
        "    initialize(model, initname)\n",
        "\n",
        "    for data in datasets[1:2]:\n",
        "\n",
        "      finalmodel = run_train(model=model, traindata=data[0], testdata=data[1],\n",
        "                epochsnum=epochsnum, save_epochs=save_epochs)\n",
        "      \n",
        "      negativemodel = negative_model(finalmodel)\n",
        "      run_train(model=negativemodel, traindata=data[0], testdata=data[1],\n",
        "                epochsnum=epochsnum, save_epochs=save_epochs)\n",
        "      \n",
        "      distortedmodel = distorted_model(finalmodel)\n",
        "      run_train(model=distortedmodel, traindata=data[0], testdata=data[1],\n",
        "                epochsnum=epochsnum, save_epochs=save_epochs)\n"
      ],
      "metadata": {
        "id": "wpYB7fpVAtJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kccVOvntXG_z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}